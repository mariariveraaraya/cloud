[
  {
    "objectID": "cloud_concepts/python_query_synapse_views.html",
    "href": "cloud_concepts/python_query_synapse_views.html",
    "title": "Querying Synapse from VS Code using Python",
    "section": "",
    "text": "This notebook provides a guide for querying and analyzing available views in Azure Synapse Analytics directly from Visual Studio Code (VS Code) or your preferred Integrated Development Environment (IDE) using Python. By following the instructions in this notebook, you will learn how to establish a connection to your Synapse workspace, execute SQL queries against the available views, and retrieve the results. Search for  for locations where you need to input the appropriate values based on your use case.\nPrerequisites:\n\nData Reader permissions in the storage account you are trying to access.\nAn installation of Python available in your local machine\nAn IDE (like VSCode)\n\n\n# Install pyodbc for SQL Server connectivity  \n\n%pip install pyodbc\n\n\n# Import libraries for SQL Server connectivity and data manipulation\n\nimport pyodbc\nimport pandas as pd\n\n\n# Define connection parameters. Once you run this cell a new tab in your browser will open asking you to login to your Azure account. Once you login, you will be able to connect to the database.\n# TODO: Replace the server name and username with your own values\n\nserver = \"sbox-synapse-prd-ondemand.sql.azuresynapse.net\" # make sure to change it to your server name\ndatabase = \"synapse_od\"\nusername = \"myemail@des.qld.gov.au\" # Replace with your des your email here\nAuthentication = \"ActiveDirectoryInteractive\"\ndriver = \"{ODBC Driver 17 for SQL Server}\"\n\n\n# Connect to SQL Server using pyodbc and the parameters above. Once you run this cell a new tab in your browser will open asking you to login to your Azure account. Once you login, you will be able to connect to the database.\n\n\nconn_str = f\"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};Authentication={Authentication};\"\nconn = pyodbc.connect(conn_str)\n\n\n# Query the database and load the results into a dataframe\n# Here we want to retrieve the first 100 observations from the tide view in sbox_curated. Check out the next cell to understand how to get the name of available views\n# TODO: Replace the query with your own query\n\nquery = \"SELECT TOP (100) [timestamp] ,[lowest_astronomical_tide_datum_reading] FROM [sbox_curated].[tide]\" \n\ndf = pd.read_sql(query, conn)\n\nfor index, row in df.iterrows():\n    # Process each row as needed\n    print(row)\n\n\n# To get the list of available views in the database, we can use the following code:\n\n# Create a cursor object\ncursor = conn.cursor()\n\n# Get the list of available tables\ntables = cursor.tables()\n\n# Iterate over the tables and print their names\nfor table in tables:\n    print(table.table_name)\n\n\n# Close the connection to the database when done\n\nconn.close()",
    "crumbs": [
      "Tutorials",
      "Querying Synapse from VS Code using Python"
    ]
  },
  {
    "objectID": "cloud_concepts/python_query_synapse_views.html#querying-synapse-from-vs-code-using-python",
    "href": "cloud_concepts/python_query_synapse_views.html#querying-synapse-from-vs-code-using-python",
    "title": "Querying Synapse from VS Code using Python",
    "section": "",
    "text": "This notebook provides a guide for querying and analyzing available views in Azure Synapse Analytics directly from Visual Studio Code (VS Code) or your preferred Integrated Development Environment (IDE) using Python. By following the instructions in this notebook, you will learn how to establish a connection to your Synapse workspace, execute SQL queries against the available views, and retrieve the results. Search for  for locations where you need to input the appropriate values based on your use case.\nPrerequisites:\n\nData Reader permissions in the storage account you are trying to access.\nAn installation of Python available in your local machine\nAn IDE (like VSCode)\n\n\n# Install pyodbc for SQL Server connectivity  \n\n%pip install pyodbc\n\n\n# Import libraries for SQL Server connectivity and data manipulation\n\nimport pyodbc\nimport pandas as pd\n\n\n# Define connection parameters. Once you run this cell a new tab in your browser will open asking you to login to your Azure account. Once you login, you will be able to connect to the database.\n# TODO: Replace the server name and username with your own values\n\nserver = \"sbox-synapse-prd-ondemand.sql.azuresynapse.net\" # make sure to change it to your server name\ndatabase = \"synapse_od\"\nusername = \"myemail@des.qld.gov.au\" # Replace with your des your email here\nAuthentication = \"ActiveDirectoryInteractive\"\ndriver = \"{ODBC Driver 17 for SQL Server}\"\n\n\n# Connect to SQL Server using pyodbc and the parameters above. Once you run this cell a new tab in your browser will open asking you to login to your Azure account. Once you login, you will be able to connect to the database.\n\n\nconn_str = f\"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};Authentication={Authentication};\"\nconn = pyodbc.connect(conn_str)\n\n\n# Query the database and load the results into a dataframe\n# Here we want to retrieve the first 100 observations from the tide view in sbox_curated. Check out the next cell to understand how to get the name of available views\n# TODO: Replace the query with your own query\n\nquery = \"SELECT TOP (100) [timestamp] ,[lowest_astronomical_tide_datum_reading] FROM [sbox_curated].[tide]\" \n\ndf = pd.read_sql(query, conn)\n\nfor index, row in df.iterrows():\n    # Process each row as needed\n    print(row)\n\n\n# To get the list of available views in the database, we can use the following code:\n\n# Create a cursor object\ncursor = conn.cursor()\n\n# Get the list of available tables\ntables = cursor.tables()\n\n# Iterate over the tables and print their names\nfor table in tables:\n    print(table.table_name)\n\n\n# Close the connection to the database when done\n\nconn.close()",
    "crumbs": [
      "Tutorials",
      "Querying Synapse from VS Code using Python"
    ]
  },
  {
    "objectID": "cloud_concepts/python_blob_storage_sdk.html",
    "href": "cloud_concepts/python_blob_storage_sdk.html",
    "title": "How to download files from Azure Blob Storage in the QESD Platform using Python in a local IDE",
    "section": "",
    "text": "How to download files from Azure Blob Storage in the QESD Platform using Python in a local IDE\nTo access files stored in Azure Blob Storage within the QESD Platform from a local IDE (such as Visual Studio Code), you need to set up authentication. Azure offers various authentication methods for accessing resources. In the QESD Platform, the preferred authentication method for most users is through Microsoft Entra ID (previously known as Azure Active Directory - Azure AD). Check out the BlobServiceClient class in the azure.storage.blob to explore other features such as writing and uploading blobs to a storage account.\nThe following example illustrates how to download a blob from Azure Blob Storage using the Azure SDK for Python. Search for  for locations where you need to input the appropriate values based on your use case. Please note that currently there is no suitable method available for R.\n\nPrerequisites:\n\nData Reader permissions in the storage account you are trying to access.\nAn installation of Python available in your local machine\nAn IDE (like VSCode)\n\n\n# Install required libraries. We recommend using your preferred package manager or virtual environment to install the libraries in the correct environment and avoid conflicts with other packages.\n\n# %pip install azure-identity\n# %pip install azure-storage-blob\n# %pip install pandas\n\n\n# In the code below, we import the necessary modules from the Azure SDK and create an instance of `InteractiveBrowserCredential` for interactive authentication. \n# This will prompt you to sign in interactively in the browser if no token is cached or if a device code is required. \n# We then create a `BlobServiceClient` object by providing the account URL and credential.\n\n\n# Import the necessary modules from the Azure SDK \n\nfrom azure.identity import InteractiveBrowserCredential\nfrom azure.storage.blob import BlobServiceClient\n\n# Create an instance of InteractiveBrowserCredential for interactive authentication \n# This will prompt you to sign in interactively and obtain the necessary token\n# The credential object will be used to authenticate with Azure services\n\ntry:\n    credential = InteractiveBrowserCredential()\n\n# Obtain a token from the Azure Active Directory endpoint to access the Azure Storage account\n    credential.get_token(\"https://management.azure.com/.default\")\n\n# Check if the given credential can successfully obtain a token for the specified resource\n# If an exception occurs during authentication, print the error message\n\nexcept Exception as ex:\n    print(f\"Error: {ex}\")\n\n# Create a BlobServiceClient object by providing the account URL and credential\n# The account URL is the URL of your Azure Blob Storage account\n# The credential is the authentication credential used to access the storage account\n\n\nblob_service_client = BlobServiceClient(\n    #TODO: Replace the account_url parameter with the URL of your Azure Blob Storage account, for example \"https://soilslakeuat.blob.core.windows.net\"\n    account_url=\"https://sboxlakedev.blob.core.windows.net\", \n    credential=credential\n)\n\n\n# TODO: Specify the name of the container and the blob you want to download\n\ncontainer_name = \"lake-userupload\"\nblob_name = \"wqm17\"\n\n# Get a reference to the blob by calling the get_blob_client method of the BlobServiceClient\n# Provide the container name and blob name as parameters\n\nblob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n\n\n# Download the blob by calling the download_blob method of the blob client\ndownloaded_blob = blob_client.download_blob()\n\n# TODO: Specify the local file path where you want to save the downloaded blob\n\nlocal_file_path = \"C:/Users/riveraarayam/Downloads/20231218_training_wqm17.csv\"\n\n# Open the local file in write binary mode and write the contents of the downloaded blob to the file\n\nwith open(local_file_path, \"wb\") as file:\n    file.write(downloaded_blob.readall())\n\n# Print a success message indicating that the blob has been downloaded\nprint(\"Blob downloaded successfully.\")\n\nBlob downloaded successfully.\n\n\n\n\n\nHow to read files in a Blob storage account in the QESD Platform from a local IDE using Python (without downloading)\nThis guide explains how to read files stored in a Blob storage account within the QESD Platform using Python, without the need to download the entire file. By leveraging the Azure Blob Storage SDK for Python, you can access the file’s content directly from your local IDE. This approach allows you to efficiently read and process the file’s data without the overhead of downloading the entire file. The tutorial provides step-by-step instructions on setting up the necessary authentication, accessing the Blob storage account, and reading the file as a Pandas DataFrame using Python. Search for  for locations where you need to input the appropriate values based on your use case.\n\n# Import required libraries \n\nimport pandas as pd\nfrom io import BytesIO\nfrom azure.identity import InteractiveBrowserCredential\nfrom azure.storage.blob import BlobServiceClient\n\n# Create an instance of InteractiveBrowserCredential for interactive authentication \n# This will prompt you to sign in interactively and obtain the necessary token\n# The credential object will be used to authenticate with Azure services\n\ntry:\n    credential = InteractiveBrowserCredential()\n\n# Obtain a token from the Azure Active Directory endpoint to access the Azure Storage account\n    credential.get_token(\"https://management.azure.com/.default\")\n# Check if the given credential can successfully obtain a token for the specified resource\n# If an exception occurs during authentication, print the error message\n\nexcept Exception as ex:\n    print(f\"Error: {ex}\")\n\n\n# Create a BlobServiceClient object\n\nblob_service_client = BlobServiceClient(\n    # TODO: Replace the account_url parameter with the URL of your Azure Blob Storage account, for example \"https://soilslakeuat.blob.core.windows.net\"\n    account_url=\"https://sboxlakedev.blob.core.windows.net\",\n    credential=credential\n)\n\n\n# TODO: Specify the name of the container and the blob you want to read\n\ncontainer_name = \"lake-userupload\"\nblob_name = \"wqm17\"\n\n# Get a BlobClient object for the specified blob\n\nblob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n\n# Read the blob data as bytes\nblob_data = blob_client.download_blob().readall()\n\n# Create a BytesIO object from the blob data\nbytes_io = BytesIO(blob_data)\n\n# Read the BytesIO object as a Pandas DataFrame\ndf = pd.read_csv(bytes_io)\n\n# Use the DataFrame as needed\nprint(df.head())",
    "crumbs": [
      "Tutorials",
      "How to download files from Azure Blob Storage in the QESD Platform using Python in a local IDE"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "",
    "text": "! Important: This page is in construction. We will be adding additional tutorials to this public version soon.\nThe Queensland Environmental Science Data (QESD) Platform is a collection of modern, secure, and interoperable cloud components. These components, which include infrastructure, applications, and tools, are scalable and available on-demand on a pay-as-you-go basis for data processing.\nThese tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the implementation of the Queensland Environmental Science Data (QESD) cloud Platform at the Department of Environment, Science and Innovation, and are available for self-paced learning.",
    "crumbs": [
      "Welcome!",
      "Cloud tools for scientific analyses in the QESD cloud platform"
    ]
  },
  {
    "objectID": "index.html#introduction-to-cloud-based-scientific-analysis-tools",
    "href": "index.html#introduction-to-cloud-based-scientific-analysis-tools",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Introduction to cloud-based scientific analysis tools",
    "text": "Introduction to cloud-based scientific analysis tools\nIncreasing data volumes and technological advancements are ushering us into a new era of scientific analysis. One such advancement is cloud computing, enabling users to effectively process and analyse vast amounts of data from anywhere in the world. These tutorials will introduce you to cloud-based tools employed in scientific analysis in the QESD platform. Check out our module that covers cloud fundamental concepts.",
    "crumbs": [
      "Welcome!",
      "Cloud tools for scientific analyses in the QESD cloud platform"
    ]
  },
  {
    "objectID": "index.html#access-to-data-in-the-qesd-cloud-platform",
    "href": "index.html#access-to-data-in-the-qesd-cloud-platform",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Access to data in the QESD Cloud Platform",
    "text": "Access to data in the QESD Cloud Platform\nThis section includes detailed processes on how to login from your local integrated development environment to read data from Synapse.",
    "crumbs": [
      "Welcome!",
      "Cloud tools for scientific analyses in the QESD cloud platform"
    ]
  },
  {
    "objectID": "index.html#geospatial-case-study-and-best-practice-examples",
    "href": "index.html#geospatial-case-study-and-best-practice-examples",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Geospatial case study and best practice examples",
    "text": "Geospatial case study and best practice examples\nThis section features a workflow that demonstrate how we can use cloud tools available in the QESD cloud platform to perform geopatial analyses.",
    "crumbs": [
      "Welcome!",
      "Cloud tools for scientific analyses in the QESD cloud platform"
    ]
  },
  {
    "objectID": "index.html#coming-up",
    "href": "index.html#coming-up",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Coming up:",
    "text": "Coming up:\n\nIntegrating Cloud Tools in your research\nCase Studies and Best Practice Examples\nResources and continuous learning",
    "crumbs": [
      "Welcome!",
      "Cloud tools for scientific analyses in the QESD cloud platform"
    ]
  },
  {
    "objectID": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "href": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "title": "Spatial operations using Azure and the QESD Platform",
    "section": "",
    "text": "This notebook is part of a proof of concept developed to test the capabilities in the QESD platform to process large geospatial data and replicate workflows from ArcGIS Pro. In particular, this notebook shows how to perform spatial joins and counts using two geospatial datasets.\n\n\n\nThe datasets were manually ingested as there is not a current integration between SIR or Open Data and the QESD platform. The protected areas of Queensland geospatial dataset was downloaded from SIR in geojson format and then read with geopandas locally to preserve the geometry. It was then saved as parquet format and uploaded to Blob Storage. Alternatively, the json file can be read directly with geopandas if the file is stored in Databricks Workspace, for example: example = gpd.read_file(‘/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/pa_all.json’). This dataset contains 1926 polygons (~ 7 MB).\nThe second file is the WildNet wildlife records (until 2023-09-21) sourced from Open data in gpkg format. The file was read with geopandas locally to preserve the geometry. The geometry column was transformed to string to make it compatible with the expected datatypes in parquet. It was then saved as parquet format and uploaded to blob Storage in the platform. This dataset contains ~2.2 million records (1.4 GB).\n\n\n\nIt was possible to replicate the process of joining both datasets and producing total wildlife counts per protected area and per polygon using the Python module geopandas. Further granularity (filter, select) can be achieved based on future requirements. In summary, spatial joins, total wildlife counts in 1926 polygons and 1047 protected areas (from ~1770 to 2023-09-21) including those with multipolygons were performed.\n\n\n\n\nFamiliarity with spatial data structures and objects\nFamiliarity with Databricks\nUnderstanding of Azure Data Lake Storage (ADLS)\nFamiliarity with Python\nBasic understanding of Spark\n\n\n\n\n\nMake sure you have access to a Databricks environment in the QESD platform.\nNavigate to sbox-databricks-prd\nNavigate to Workspace and open the folder platform_repo/geospatial. This folder contains several examples demonstrating the use of Azure Databricks for different geospatial analysis and operations in vector data, including building choropleth maps, spatial joins, counts and calculation of polygon areas in small and large datasets.\nStart a cluster. We recommend ‘singlenode’ as we do not need distributed computing and our datasets are small.\n\n\n# Install modules\n\n%pip install geopandas==0.14.0 # Pandas equivalent to process geospatial data\n%pip install folium==0.14.0 # To produce interactive maps\n\n\n# These libraries are pre-installed in a Databricks 12.2 LTS runtime cluster (except geopandas and folium)\n\nimport geopandas as gpd # Pandas equivalent to process geospatial data\nfrom shapely.geometry import Polygon, Point # To create geometries \nimport folium # To produce interactive maps\nfrom json import loads  # To load json files\nfrom shapely import wkt # To convert wkt geometries\nimport pandas as pd # To process data    \nimport os # To interact with the operating system\nfrom folium import plugins # To produce interactive maps\n\n\n# Import datasets from Azure Data Lake Storage (ADLS)\n\n# Get the environment name from the system environment variables\n\n\nenv = os.getenv('tfenvironmentnameshort')\n\n# Load the 'pa_all_srt.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n\nprotected_areas = (\n  spark.read\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pa_all_srt.parquet')\n)\n\n# Load the 'wildnet_pp_str.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n# The 'header' option is set to 'true' which means the first row of the dataset is considered as the header\n\nwild = (\n  spark.read\n    .option(\"header\", \"true\")\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/wildnet_pp_str.parquet')\n)\n\n\n# Convert Spark DataFrames to pandas DataFrames \n# This is necessary to convert the geometries from wkt to shapely geometries\n\npa_pd = protected_areas.toPandas()\nwild_pd = wild.toPandas()\n\n# Convert pandas DataFrames to GeoPandas DataFrames\n# This is necessary to perform spatial operations\n\npa_gdf = gpd.GeoDataFrame(pa_pd)\nwild_gpd = gpd.GeoDataFrame(wild_pd)\n\n\n# Define geometries\n\n# The 'geometry' column in the original data is a string\n# We need to convert this string representation of geometry into a Shapely geometry object\n# The 'from_wkt' function from GeoPandas is used for this conversion\n\npa_gdf['geometry2'] = gpd.GeoSeries.from_wkt(pa_gdf['geometry'])\n\n# Now, we initialize a new GeoDataFrame using the updated 'geometry2' column as the geometry\n# Note: 'geometry' is the default geometry column name in GeoPandas\npa_gdf = gpd.GeoDataFrame(pa_gdf).set_geometry(\"geometry2\")\n\n# replace string geometry representations with shapely geometries\nwild_gpd['geometry2'] = gpd.GeoSeries.from_wkt(wild_gpd['geometry'])\n\n# We repeat the same steps for the 'wild' dataset\n\nwild_gpd= gpd.GeoDataFrame(wild_gpd).set_geometry(\"geometry2\")\n\n\n# Perform spatial join on right to retain all records from observation points\n# The 'contains' operation is used to find all protected areas that contain the observation points\n\njoined_inner = gpd.sjoin(pa_gdf, wild_gpd, how='right', op='contains')\n\n\n# Count by protected area and polygon\n# The 'groupby' function is used to group the data by protected area and polygon\n\npoint_count = joined_inner.groupby(['geometry_left', 'estatename']).size().reset_index(name='total number of sightings')\n\n\n# Rejoin with protected areas\n# The 'merge' function is used to join the 'point_count' and 'pa_gdf' GeoDataFrames\nmerged_counts_pa = point_count.merge(pa_gdf, left_on = ['geometry_left'], right_on= ['geometry'], how = 'outer')\n\n# Convert to dataframe\n# The 'set_geometry' function is used to set the 'geometry2' column as the geometry column\nmerged_gdf = gpd.GeoDataFrame(merged_counts_pa).set_geometry('geometry2')\n\n\n# Calculate total number of sightings per protected area\n# The 'groupby' function is used to group the data by protected area\n# The 'aggregate' function is used to calculate the sum of the 'total number of sightings' column\n# The 'rename' function is used to rename the 'total number of sightings' column to 'aggregated sightings'\n\naggregation_functions = {'total number of sightings': 'sum'}\nmerged_agg = merged_gdf.groupby(merged_gdf['estatename_y']).aggregate(aggregation_functions)\nmerged_agg = merged_agg.rename(columns={\"total number of sightings\": \"aggregated sightings\"})\n\n\n# Set correct indexes to join aggregations with previous dataframe\n# The 'set_index' function is used to set the 'estatename_y' column as the index\n# The 'join' function is used to join the 'merged_gdf_index' and 'merged_agg' GeoDataFrames\n# The 'reset_index' function is used to reset the index of the GeoDataFrame\n\nmerged_gdf_index = merged_gdf.set_index('estatename_y')\nmerged = merged_gdf_index.join(merged_agg).reset_index()\n\n\n# Merge resulting dataframe with raw protected areas to include polygons with zero sightings\n# The 'merge' function is used to join the 'merged' and 'pa_gdf' GeoDataFrames\n# The 'left_on' and 'right_on' parameters are used to specify the join columns\n\nfinal_merged = merged.merge(pa_gdf, left_on= 'geometry2', right_on= 'geometry2', how = 'outer')\n\n# Include polygons with zero sighthings. Set zero in the polygon count as they show as NAs and replace NA's in aggregated sightings in those polygons with the value already calculated.\n# The 'fillna' function is used to replace all NaN values with 0\n# The 'groupby' function is used to group the data by protected area\n# The 'apply' function is used to apply a lambda function to the 'aggregated sightings' column\n# The lambda function is used to replace all NaN values with the maximum value in the 'aggregated sightings' column\n\nfinal_merged['total number of sightings'] = final_merged['total number of sightings'].fillna(0)\nfinal_merged['aggregated sightings'] = final_merged.groupby('estatename')['aggregated sightings'].apply(lambda x: x.fillna(x.max()))\nfinal_merged['aggregated sightings'] = final_merged['aggregated sightings'].fillna(0)\nfinal_merged = final_merged.drop(['estatename_x', 'geometry_x', 'geometry_y', 'geometry_left', 'estatename_y'], axis=1)\n\n\n# Test with three protected areas to compare with ArcGIS Pro\n# The 'loc' function is used to select the rows where the 'estatename' column is equal to 'Great Sandy National Park', 'Minerva Hills National Park', or 'Tinana Creek Conservation Park'\n\nfiltered_df = final_merged.loc[(final_merged['estatename'] == 'Great Sandy National Park') | (final_merged['estatename'] == 'Minerva Hills National Park') | (final_merged['estatename'] == 'Tinana Creek Conservation Park')]\n\nfiltered_df\n\n\n# Use folium to create a map. Only three protected areas are included for demonstration\n# The 'Map' function is used to create a map\n# The 'location' parameter is used to specify the center of the map\n# The 'zoom_start' parameter is used to specify the zoom level of the map\n\nm = folium.Map(location = [-24.0807, 148.0641], zoom_start = 6)\n\n\n# Use subset of main dataframe to build map\n\nfinal_merged_gdf = gpd.GeoDataFrame(filtered_df.set_geometry('geometry2')) \nfinal_merged_gdf = final_merged_gdf.set_crs('EPSG:4283')\n\n\n# Convert to geojson so plugin from folium can be used\n# The 'to_json' function is used to convert the GeoDataFrame to a GeoJSON object\n\npoints_gjson= folium.features.GeoJson(final_merged_gdf, name=\"wildlife counts\")\npoints_gjson.add_to(m)\n\n\n# Use folium plugin to create tooltip\n# The 'plugins.MarkerCluster' function is used to create a marker cluster\n# The 'add_to' function is used to add the marker cluster to the map\nfolium.features.GeoJson(final_merged_gdf,  \n                        name='Labels',\n                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n                        tooltip=folium.features.GeoJsonTooltip(fields=['aggregated sightings', 'estatename', 'total number of sightings'], \n                                                                aliases = ['Total number of sightings in protected area', 'Protected area', 'Number of sightings in polygon'],\n                                                                labels=True,\n                                                                sticky=False\n                                                                            )\n                       ).add_to(m)\n\nm\n\n\n# Save map to workspace - replace with your own path\n\n#m.save(outfile='/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/sandy_minerva_tinana.html')\n\n\n# Check number of polygons is correct\n# The 'count' function is used to count the number of rows in the GeoDataFrame\n\n# To store in parquet - geometry column needs to be changed\n\nmerged_pq = final_merged.copy()\n\nmerged_pq['geometry2'] = merged_pq['geometry2'].astype(str)\n\nmerged_pq = merged_pq.rename(columns = {'total number of sightings': 'number of sightings in polygon', 'aggregated sightings':'total number of sightings in protected area', 'geometry2': 'geometry'})\n\nmerged_pq.count()\n\n\n# Browse total number of sightings per protected area\n# The 'drop_duplicates' function is used to drop duplicate rows based on the 'estatename' and 'total number of sightings in protected area' columns\n# The 'createDataFrame' function is used to create a Spark DataFrame from a pandas DataFrame\n\n\nmerged_pq_sum = merged_pq[['estatename', 'total number of sightings in protected area']].drop_duplicates()\ndisplay(spark.createDataFrame(merged_pq_sum))\n\n\n# To store csv\n  \n #spark.createDataFrame(test).repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial/sandy_minerva_test.csv')\n\n\n# Write to blob \n# The 'write' function is used to write the Spark DataFrame to the specified file path\n\nspark.createDataFrame(merged_pq).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pr_areas_counts.parquet')",
    "crumbs": [
      "Tutorials",
      "Spatial operations using Azure and the QESD Platform"
    ]
  },
  {
    "objectID": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html#spatial-operations-using-azure-and-the-qesd-platform",
    "href": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html#spatial-operations-using-azure-and-the-qesd-platform",
    "title": "Spatial operations using Azure and the QESD Platform",
    "section": "",
    "text": "This notebook is part of a proof of concept developed to test the capabilities in the QESD platform to process large geospatial data and replicate workflows from ArcGIS Pro. In particular, this notebook shows how to perform spatial joins and counts using two geospatial datasets.\n\n\n\nThe datasets were manually ingested as there is not a current integration between SIR or Open Data and the QESD platform. The protected areas of Queensland geospatial dataset was downloaded from SIR in geojson format and then read with geopandas locally to preserve the geometry. It was then saved as parquet format and uploaded to Blob Storage. Alternatively, the json file can be read directly with geopandas if the file is stored in Databricks Workspace, for example: example = gpd.read_file(‘/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/pa_all.json’). This dataset contains 1926 polygons (~ 7 MB).\nThe second file is the WildNet wildlife records (until 2023-09-21) sourced from Open data in gpkg format. The file was read with geopandas locally to preserve the geometry. The geometry column was transformed to string to make it compatible with the expected datatypes in parquet. It was then saved as parquet format and uploaded to blob Storage in the platform. This dataset contains ~2.2 million records (1.4 GB).\n\n\n\nIt was possible to replicate the process of joining both datasets and producing total wildlife counts per protected area and per polygon using the Python module geopandas. Further granularity (filter, select) can be achieved based on future requirements. In summary, spatial joins, total wildlife counts in 1926 polygons and 1047 protected areas (from ~1770 to 2023-09-21) including those with multipolygons were performed.\n\n\n\n\nFamiliarity with spatial data structures and objects\nFamiliarity with Databricks\nUnderstanding of Azure Data Lake Storage (ADLS)\nFamiliarity with Python\nBasic understanding of Spark\n\n\n\n\n\nMake sure you have access to a Databricks environment in the QESD platform.\nNavigate to sbox-databricks-prd\nNavigate to Workspace and open the folder platform_repo/geospatial. This folder contains several examples demonstrating the use of Azure Databricks for different geospatial analysis and operations in vector data, including building choropleth maps, spatial joins, counts and calculation of polygon areas in small and large datasets.\nStart a cluster. We recommend ‘singlenode’ as we do not need distributed computing and our datasets are small.\n\n\n# Install modules\n\n%pip install geopandas==0.14.0 # Pandas equivalent to process geospatial data\n%pip install folium==0.14.0 # To produce interactive maps\n\n\n# These libraries are pre-installed in a Databricks 12.2 LTS runtime cluster (except geopandas and folium)\n\nimport geopandas as gpd # Pandas equivalent to process geospatial data\nfrom shapely.geometry import Polygon, Point # To create geometries \nimport folium # To produce interactive maps\nfrom json import loads  # To load json files\nfrom shapely import wkt # To convert wkt geometries\nimport pandas as pd # To process data    \nimport os # To interact with the operating system\nfrom folium import plugins # To produce interactive maps\n\n\n# Import datasets from Azure Data Lake Storage (ADLS)\n\n# Get the environment name from the system environment variables\n\n\nenv = os.getenv('tfenvironmentnameshort')\n\n# Load the 'pa_all_srt.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n\nprotected_areas = (\n  spark.read\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pa_all_srt.parquet')\n)\n\n# Load the 'wildnet_pp_str.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n# The 'header' option is set to 'true' which means the first row of the dataset is considered as the header\n\nwild = (\n  spark.read\n    .option(\"header\", \"true\")\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/wildnet_pp_str.parquet')\n)\n\n\n# Convert Spark DataFrames to pandas DataFrames \n# This is necessary to convert the geometries from wkt to shapely geometries\n\npa_pd = protected_areas.toPandas()\nwild_pd = wild.toPandas()\n\n# Convert pandas DataFrames to GeoPandas DataFrames\n# This is necessary to perform spatial operations\n\npa_gdf = gpd.GeoDataFrame(pa_pd)\nwild_gpd = gpd.GeoDataFrame(wild_pd)\n\n\n# Define geometries\n\n# The 'geometry' column in the original data is a string\n# We need to convert this string representation of geometry into a Shapely geometry object\n# The 'from_wkt' function from GeoPandas is used for this conversion\n\npa_gdf['geometry2'] = gpd.GeoSeries.from_wkt(pa_gdf['geometry'])\n\n# Now, we initialize a new GeoDataFrame using the updated 'geometry2' column as the geometry\n# Note: 'geometry' is the default geometry column name in GeoPandas\npa_gdf = gpd.GeoDataFrame(pa_gdf).set_geometry(\"geometry2\")\n\n# replace string geometry representations with shapely geometries\nwild_gpd['geometry2'] = gpd.GeoSeries.from_wkt(wild_gpd['geometry'])\n\n# We repeat the same steps for the 'wild' dataset\n\nwild_gpd= gpd.GeoDataFrame(wild_gpd).set_geometry(\"geometry2\")\n\n\n# Perform spatial join on right to retain all records from observation points\n# The 'contains' operation is used to find all protected areas that contain the observation points\n\njoined_inner = gpd.sjoin(pa_gdf, wild_gpd, how='right', op='contains')\n\n\n# Count by protected area and polygon\n# The 'groupby' function is used to group the data by protected area and polygon\n\npoint_count = joined_inner.groupby(['geometry_left', 'estatename']).size().reset_index(name='total number of sightings')\n\n\n# Rejoin with protected areas\n# The 'merge' function is used to join the 'point_count' and 'pa_gdf' GeoDataFrames\nmerged_counts_pa = point_count.merge(pa_gdf, left_on = ['geometry_left'], right_on= ['geometry'], how = 'outer')\n\n# Convert to dataframe\n# The 'set_geometry' function is used to set the 'geometry2' column as the geometry column\nmerged_gdf = gpd.GeoDataFrame(merged_counts_pa).set_geometry('geometry2')\n\n\n# Calculate total number of sightings per protected area\n# The 'groupby' function is used to group the data by protected area\n# The 'aggregate' function is used to calculate the sum of the 'total number of sightings' column\n# The 'rename' function is used to rename the 'total number of sightings' column to 'aggregated sightings'\n\naggregation_functions = {'total number of sightings': 'sum'}\nmerged_agg = merged_gdf.groupby(merged_gdf['estatename_y']).aggregate(aggregation_functions)\nmerged_agg = merged_agg.rename(columns={\"total number of sightings\": \"aggregated sightings\"})\n\n\n# Set correct indexes to join aggregations with previous dataframe\n# The 'set_index' function is used to set the 'estatename_y' column as the index\n# The 'join' function is used to join the 'merged_gdf_index' and 'merged_agg' GeoDataFrames\n# The 'reset_index' function is used to reset the index of the GeoDataFrame\n\nmerged_gdf_index = merged_gdf.set_index('estatename_y')\nmerged = merged_gdf_index.join(merged_agg).reset_index()\n\n\n# Merge resulting dataframe with raw protected areas to include polygons with zero sightings\n# The 'merge' function is used to join the 'merged' and 'pa_gdf' GeoDataFrames\n# The 'left_on' and 'right_on' parameters are used to specify the join columns\n\nfinal_merged = merged.merge(pa_gdf, left_on= 'geometry2', right_on= 'geometry2', how = 'outer')\n\n# Include polygons with zero sighthings. Set zero in the polygon count as they show as NAs and replace NA's in aggregated sightings in those polygons with the value already calculated.\n# The 'fillna' function is used to replace all NaN values with 0\n# The 'groupby' function is used to group the data by protected area\n# The 'apply' function is used to apply a lambda function to the 'aggregated sightings' column\n# The lambda function is used to replace all NaN values with the maximum value in the 'aggregated sightings' column\n\nfinal_merged['total number of sightings'] = final_merged['total number of sightings'].fillna(0)\nfinal_merged['aggregated sightings'] = final_merged.groupby('estatename')['aggregated sightings'].apply(lambda x: x.fillna(x.max()))\nfinal_merged['aggregated sightings'] = final_merged['aggregated sightings'].fillna(0)\nfinal_merged = final_merged.drop(['estatename_x', 'geometry_x', 'geometry_y', 'geometry_left', 'estatename_y'], axis=1)\n\n\n# Test with three protected areas to compare with ArcGIS Pro\n# The 'loc' function is used to select the rows where the 'estatename' column is equal to 'Great Sandy National Park', 'Minerva Hills National Park', or 'Tinana Creek Conservation Park'\n\nfiltered_df = final_merged.loc[(final_merged['estatename'] == 'Great Sandy National Park') | (final_merged['estatename'] == 'Minerva Hills National Park') | (final_merged['estatename'] == 'Tinana Creek Conservation Park')]\n\nfiltered_df\n\n\n# Use folium to create a map. Only three protected areas are included for demonstration\n# The 'Map' function is used to create a map\n# The 'location' parameter is used to specify the center of the map\n# The 'zoom_start' parameter is used to specify the zoom level of the map\n\nm = folium.Map(location = [-24.0807, 148.0641], zoom_start = 6)\n\n\n# Use subset of main dataframe to build map\n\nfinal_merged_gdf = gpd.GeoDataFrame(filtered_df.set_geometry('geometry2')) \nfinal_merged_gdf = final_merged_gdf.set_crs('EPSG:4283')\n\n\n# Convert to geojson so plugin from folium can be used\n# The 'to_json' function is used to convert the GeoDataFrame to a GeoJSON object\n\npoints_gjson= folium.features.GeoJson(final_merged_gdf, name=\"wildlife counts\")\npoints_gjson.add_to(m)\n\n\n# Use folium plugin to create tooltip\n# The 'plugins.MarkerCluster' function is used to create a marker cluster\n# The 'add_to' function is used to add the marker cluster to the map\nfolium.features.GeoJson(final_merged_gdf,  \n                        name='Labels',\n                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n                        tooltip=folium.features.GeoJsonTooltip(fields=['aggregated sightings', 'estatename', 'total number of sightings'], \n                                                                aliases = ['Total number of sightings in protected area', 'Protected area', 'Number of sightings in polygon'],\n                                                                labels=True,\n                                                                sticky=False\n                                                                            )\n                       ).add_to(m)\n\nm\n\n\n# Save map to workspace - replace with your own path\n\n#m.save(outfile='/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/sandy_minerva_tinana.html')\n\n\n# Check number of polygons is correct\n# The 'count' function is used to count the number of rows in the GeoDataFrame\n\n# To store in parquet - geometry column needs to be changed\n\nmerged_pq = final_merged.copy()\n\nmerged_pq['geometry2'] = merged_pq['geometry2'].astype(str)\n\nmerged_pq = merged_pq.rename(columns = {'total number of sightings': 'number of sightings in polygon', 'aggregated sightings':'total number of sightings in protected area', 'geometry2': 'geometry'})\n\nmerged_pq.count()\n\n\n# Browse total number of sightings per protected area\n# The 'drop_duplicates' function is used to drop duplicate rows based on the 'estatename' and 'total number of sightings in protected area' columns\n# The 'createDataFrame' function is used to create a Spark DataFrame from a pandas DataFrame\n\n\nmerged_pq_sum = merged_pq[['estatename', 'total number of sightings in protected area']].drop_duplicates()\ndisplay(spark.createDataFrame(merged_pq_sum))\n\n\n# To store csv\n  \n #spark.createDataFrame(test).repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial/sandy_minerva_test.csv')\n\n\n# Write to blob \n# The 'write' function is used to write the Spark DataFrame to the specified file path\n\nspark.createDataFrame(merged_pq).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pr_areas_counts.parquet')",
    "crumbs": [
      "Tutorials",
      "Spatial operations using Azure and the QESD Platform"
    ]
  }
]