[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "cloud_concepts/python_query_synapse_views.html",
    "href": "cloud_concepts/python_query_synapse_views.html",
    "title": "Querying Synapse from VS Code using Python",
    "section": "",
    "text": "This notebook provides a guide for querying and analyzing available views in Azure Synapse Analytics directly from Visual Studio Code (VS Code) or your preferred Integrated Development Environment (IDE) using Python. By following the instructions in this notebook, you will learn how to establish a connection to your Synapse workspace, execute SQL queries against the available views, and retrieve the results. Search for  for locations where you need to input the appropriate values based on your use case.\n\nPrerequisites:\n\nData Reader permissions in the storage account you are trying to access.\nAn installation of Python available in your local machine\nAn IDE (like VSCode)\n\n\n# Install pyodbc for SQL Server connectivity  \n\n%pip install pyodbc\n\n\n# Import libraries for SQL Server connectivity and data manipulation\n\nimport pyodbc\nimport pandas as pd\n\n\n# Define connection parameters. Once you run this cell a new tab in your browser will open asking you to login to your Azure account. Once you login, you will be able to connect to the database.\n# TODO: Replace the server name and username with your own values\n\nserver = \"sbox-synapse-prd-ondemand.sql.azuresynapse.net\" # make sure to change it to your server name\ndatabase = \"synapse_od\"\nusername = \"myemail@des.qld.gov.au\" # Replace with your des your email here\nAuthentication = \"ActiveDirectoryInteractive\"\ndriver = \"{ODBC Driver 17 for SQL Server}\"\n\n\n# Connect to SQL Server using pyodbc and the parameters above. Once you run this cell a new tab in your browser will open asking you to login to your Azure account. Once you login, you will be able to connect to the database.\n\n\nconn_str = f\"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};Authentication={Authentication};\"\nconn = pyodbc.connect(conn_str)\n\n\n# Query the database and load the results into a dataframe\n# Here we want to retrieve the first 100 observations from the tide view in sbox_curated. Check out the next cell to understand how to get the name of available views\n# TODO: Replace the query with your own query\n\nquery = \"SELECT TOP (100) [timestamp] ,[lowest_astronomical_tide_datum_reading] FROM [sbox_curated].[tide]\" \n\ndf = pd.read_sql(query, conn)\n\nfor index, row in df.iterrows():\n    # Process each row as needed\n    print(row)\n\n\n# To get the list of available views in the database, we can use the following code:\n\n# Create a cursor object\ncursor = conn.cursor()\n\n# Get the list of available tables\ntables = cursor.tables()\n\n# Iterate over the tables and print their names\nfor table in tables:\n    print(table.table_name)\n\n\n# Close the connection to the database when done\n\nconn.close()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "",
    "text": "! Important: This page is in construction. We will be adding additional tutorials to this public version soon.\nThe Queensland Environmental Science Data (QESD) Platform is a collection of modern, secure, and interoperable cloud components. These components, which include infrastructure, applications, and tools, are scalable and available on-demand on a pay-as-you-go basis for data processing.\nThese tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the implementation of the Queensland Environmental Science Data (QESD) cloud Platform at the Department of Environment, Science and Innovation, and are available for self-paced learning."
  },
  {
    "objectID": "index.html#introduction-to-cloud-based-scientific-analysis-tools",
    "href": "index.html#introduction-to-cloud-based-scientific-analysis-tools",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Introduction to cloud-based scientific analysis tools",
    "text": "Introduction to cloud-based scientific analysis tools\nIncreasing data volumes and technological advancements are ushering us into a new era of scientific analysis. One such advancement is cloud computing, enabling users to effectively process and analyse vast amounts of data from anywhere in the world. These tutorials will introduce you to cloud-based tools employed in scientific analysis in the QESD platform. Check out our module that covers cloud fundamental concepts."
  },
  {
    "objectID": "index.html#access-to-data-in-the-qesd-cloud-platform",
    "href": "index.html#access-to-data-in-the-qesd-cloud-platform",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Access to data in the QESD Cloud Platform",
    "text": "Access to data in the QESD Cloud Platform\nThis section includes detailed processes on how to login from your local integrated development environment to read data from Synapse."
  },
  {
    "objectID": "index.html#geospatial-case-study-and-best-practice-examples",
    "href": "index.html#geospatial-case-study-and-best-practice-examples",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Geospatial case study and best practice examples",
    "text": "Geospatial case study and best practice examples\nThis section features a workflow that demonstrate how we can use cloud tools available in the QESD cloud platform to perform geopatial analyses."
  },
  {
    "objectID": "index.html#coming-up",
    "href": "index.html#coming-up",
    "title": "Cloud tools for scientific analyses in the QESD cloud platform",
    "section": "Coming up:",
    "text": "Coming up:\n\nIntegrating Cloud Tools in your research\nCase Studies and Best Practice Examples\nResources and continuous learning"
  },
  {
    "objectID": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "href": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "title": "Spatial operations using Azure and the QESD Platform",
    "section": "",
    "text": "Description\nThis notebook is part of a proof of concept developed to test the capabilities in the QESD platform to process large geospatial data and replicate workflows from ArcGIS Pro. In particular, this notebook shows how to perform spatial joins and counts using two geospatial datasets.\n\n\nDatasets and methods\nThe datasets were manually ingested as there is not a current integration between SIR or Open Data and the QESD platform. The protected areas of Queensland geospatial dataset was downloaded from SIR in geojson format and then read with geopandas locally to preserve the geometry. It was then saved as parquet format and uploaded to Blob Storage. Alternatively, the json file can be read directly with geopandas if the file is stored in Databricks Workspace, for example: example = gpd.read_file(‘/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/pa_all.json’). This dataset contains 1926 polygons (~ 7 MB).\nThe second file is the WildNet wildlife records (until 2023-09-21) sourced from Open data in gpkg format. The file was read with geopandas locally to preserve the geometry. The geometry column was transformed to string to make it compatible with the expected datatypes in parquet. It was then saved as parquet format and uploaded to blob Storage in the platform. This dataset contains ~2.2 million records (1.4 GB).\n\n\nFindings\nIt was possible to replicate the process of joining both datasets and producing total wildlife counts per protected area and per polygon using the Python module geopandas. Further granularity (filter, select) can be achieved based on future requirements. In summary, spatial joins, total wildlife counts in 1926 polygons and 1047 protected areas (from ~1770 to 2023-09-21) including those with multipolygons were performed.\n\n\nPrerequisites\n\nFamiliarity with spatial data structures and objects\nFamiliarity with Databricks\nUnderstanding of Azure Data Lake Storage (ADLS)\nFamiliarity with Python\nBasic understanding of Spark\n\n\n\nSetup\n\nMake sure you have access to a Databricks environment in the QESD platform.\nNavigate to sbox-databricks-prd\nNavigate to Workspace and open the folder platform_repo/geospatial. This folder contains several examples demonstrating the use of Azure Databricks for different geospatial analysis and operations in vector data, including building choropleth maps, spatial joins, counts and calculation of polygon areas in small and large datasets.\nStart a cluster. We recommend ‘singlenode’ as we do not need distributed computing and our datasets are small.\n\n\n# Install modules\n\n%pip install geopandas==0.14.0 # Pandas equivalent to process geospatial data\n%pip install folium==0.14.0 # To produce interactive maps\n\n\n# These libraries are pre-installed in a Databricks 12.2 LTS runtime cluster (except geopandas and folium)\n\nimport geopandas as gpd # Pandas equivalent to process geospatial data\nfrom shapely.geometry import Polygon, Point # To create geometries \nimport folium # To produce interactive maps\nfrom json import loads  # To load json files\nfrom shapely import wkt # To convert wkt geometries\nimport pandas as pd # To process data    \nimport os # To interact with the operating system\nfrom folium import plugins # To produce interactive maps\n\n\n# Import datasets from Azure Data Lake Storage (ADLS)\n\n# Get the environment name from the system environment variables\n\n\nenv = os.getenv('tfenvironmentnameshort')\n\n# Load the 'pa_all_srt.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n\nprotected_areas = (\n  spark.read\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pa_all_srt.parquet')\n)\n\n# Load the 'wildnet_pp_str.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n# The 'header' option is set to 'true' which means the first row of the dataset is considered as the header\n\nwild = (\n  spark.read\n    .option(\"header\", \"true\")\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/wildnet_pp_str.parquet')\n)\n\n\n# Convert Spark DataFrames to pandas DataFrames \n# This is necessary to convert the geometries from wkt to shapely geometries\n\npa_pd = protected_areas.toPandas()\nwild_pd = wild.toPandas()\n\n# Convert pandas DataFrames to GeoPandas DataFrames\n# This is necessary to perform spatial operations\n\npa_gdf = gpd.GeoDataFrame(pa_pd)\nwild_gpd = gpd.GeoDataFrame(wild_pd)\n\n\n# Define geometries\n\n# The 'geometry' column in the original data is a string\n# We need to convert this string representation of geometry into a Shapely geometry object\n# The 'from_wkt' function from GeoPandas is used for this conversion\n\npa_gdf['geometry2'] = gpd.GeoSeries.from_wkt(pa_gdf['geometry'])\n\n# Now, we initialize a new GeoDataFrame using the updated 'geometry2' column as the geometry\n# Note: 'geometry' is the default geometry column name in GeoPandas\npa_gdf = gpd.GeoDataFrame(pa_gdf).set_geometry(\"geometry2\")\n\n# replace string geometry representations with shapely geometries\nwild_gpd['geometry2'] = gpd.GeoSeries.from_wkt(wild_gpd['geometry'])\n\n# We repeat the same steps for the 'wild' dataset\n\nwild_gpd= gpd.GeoDataFrame(wild_gpd).set_geometry(\"geometry2\")\n\n\n# Perform spatial join on right to retain all records from observation points\n# The 'contains' operation is used to find all protected areas that contain the observation points\n\njoined_inner = gpd.sjoin(pa_gdf, wild_gpd, how='right', op='contains')\n\n\n# Count by protected area and polygon\n# The 'groupby' function is used to group the data by protected area and polygon\n\npoint_count = joined_inner.groupby(['geometry_left', 'estatename']).size().reset_index(name='total number of sightings')\n\n\n# Rejoin with protected areas\n# The 'merge' function is used to join the 'point_count' and 'pa_gdf' GeoDataFrames\nmerged_counts_pa = point_count.merge(pa_gdf, left_on = ['geometry_left'], right_on= ['geometry'], how = 'outer')\n\n# Convert to dataframe\n# The 'set_geometry' function is used to set the 'geometry2' column as the geometry column\nmerged_gdf = gpd.GeoDataFrame(merged_counts_pa).set_geometry('geometry2')\n\n\n# Calculate total number of sightings per protected area\n# The 'groupby' function is used to group the data by protected area\n# The 'aggregate' function is used to calculate the sum of the 'total number of sightings' column\n# The 'rename' function is used to rename the 'total number of sightings' column to 'aggregated sightings'\n\naggregation_functions = {'total number of sightings': 'sum'}\nmerged_agg = merged_gdf.groupby(merged_gdf['estatename_y']).aggregate(aggregation_functions)\nmerged_agg = merged_agg.rename(columns={\"total number of sightings\": \"aggregated sightings\"})\n\n\n# Set correct indexes to join aggregations with previous dataframe\n# The 'set_index' function is used to set the 'estatename_y' column as the index\n# The 'join' function is used to join the 'merged_gdf_index' and 'merged_agg' GeoDataFrames\n# The 'reset_index' function is used to reset the index of the GeoDataFrame\n\nmerged_gdf_index = merged_gdf.set_index('estatename_y')\nmerged = merged_gdf_index.join(merged_agg).reset_index()\n\n\n# Merge resulting dataframe with raw protected areas to include polygons with zero sightings\n# The 'merge' function is used to join the 'merged' and 'pa_gdf' GeoDataFrames\n# The 'left_on' and 'right_on' parameters are used to specify the join columns\n\nfinal_merged = merged.merge(pa_gdf, left_on= 'geometry2', right_on= 'geometry2', how = 'outer')\n\n# Include polygons with zero sighthings. Set zero in the polygon count as they show as NAs and replace NA's in aggregated sightings in those polygons with the value already calculated.\n# The 'fillna' function is used to replace all NaN values with 0\n# The 'groupby' function is used to group the data by protected area\n# The 'apply' function is used to apply a lambda function to the 'aggregated sightings' column\n# The lambda function is used to replace all NaN values with the maximum value in the 'aggregated sightings' column\n\nfinal_merged['total number of sightings'] = final_merged['total number of sightings'].fillna(0)\nfinal_merged['aggregated sightings'] = final_merged.groupby('estatename')['aggregated sightings'].apply(lambda x: x.fillna(x.max()))\nfinal_merged['aggregated sightings'] = final_merged['aggregated sightings'].fillna(0)\nfinal_merged = final_merged.drop(['estatename_x', 'geometry_x', 'geometry_y', 'geometry_left', 'estatename_y'], axis=1)\n\n\n# Test with three protected areas to compare with ArcGIS Pro\n# The 'loc' function is used to select the rows where the 'estatename' column is equal to 'Great Sandy National Park', 'Minerva Hills National Park', or 'Tinana Creek Conservation Park'\n\nfiltered_df = final_merged.loc[(final_merged['estatename'] == 'Great Sandy National Park') | (final_merged['estatename'] == 'Minerva Hills National Park') | (final_merged['estatename'] == 'Tinana Creek Conservation Park')]\n\nfiltered_df\n\n\n# Use folium to create a map. Only three protected areas are included for demonstration\n# The 'Map' function is used to create a map\n# The 'location' parameter is used to specify the center of the map\n# The 'zoom_start' parameter is used to specify the zoom level of the map\n\nm = folium.Map(location = [-24.0807, 148.0641], zoom_start = 6)\n\n\n# Use subset of main dataframe to build map\n\nfinal_merged_gdf = gpd.GeoDataFrame(filtered_df.set_geometry('geometry2')) \nfinal_merged_gdf = final_merged_gdf.set_crs('EPSG:4283')\n\n\n# Convert to geojson so plugin from folium can be used\n# The 'to_json' function is used to convert the GeoDataFrame to a GeoJSON object\n\npoints_gjson= folium.features.GeoJson(final_merged_gdf, name=\"wildlife counts\")\npoints_gjson.add_to(m)\n\n\n# Use folium plugin to create tooltip\n# The 'plugins.MarkerCluster' function is used to create a marker cluster\n# The 'add_to' function is used to add the marker cluster to the map\nfolium.features.GeoJson(final_merged_gdf,  \n                        name='Labels',\n                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n                        tooltip=folium.features.GeoJsonTooltip(fields=['aggregated sightings', 'estatename', 'total number of sightings'], \n                                                                aliases = ['Total number of sightings in protected area', 'Protected area', 'Number of sightings in polygon'],\n                                                                labels=True,\n                                                                sticky=False\n                                                                            )\n                       ).add_to(m)\n\nm\n\n\n# Save map to workspace - replace with your own path\n\n#m.save(outfile='/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/sandy_minerva_tinana.html')\n\n\n# Check number of polygons is correct\n# The 'count' function is used to count the number of rows in the GeoDataFrame\n\n# To store in parquet - geometry column needs to be changed\n\nmerged_pq = final_merged.copy()\n\nmerged_pq['geometry2'] = merged_pq['geometry2'].astype(str)\n\nmerged_pq = merged_pq.rename(columns = {'total number of sightings': 'number of sightings in polygon', 'aggregated sightings':'total number of sightings in protected area', 'geometry2': 'geometry'})\n\nmerged_pq.count()\n\n\n# Browse total number of sightings per protected area\n# The 'drop_duplicates' function is used to drop duplicate rows based on the 'estatename' and 'total number of sightings in protected area' columns\n# The 'createDataFrame' function is used to create a Spark DataFrame from a pandas DataFrame\n\n\nmerged_pq_sum = merged_pq[['estatename', 'total number of sightings in protected area']].drop_duplicates()\ndisplay(spark.createDataFrame(merged_pq_sum))\n\n\n# To store csv\n  \n #spark.createDataFrame(test).repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial/sandy_minerva_test.csv')\n\n\n# Write to blob \n# The 'write' function is used to write the Spark DataFrame to the specified file path\n\nspark.createDataFrame(merged_pq).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pr_areas_counts.parquet')"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Datasets\nSlides"
  }
]