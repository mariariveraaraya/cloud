[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "href": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "title": "Spatial operations using Azure and the QESD Platform",
    "section": "",
    "text": "Description\nThis notebook is part of a proof of concept developed to test the capabilities in the QESD platform to process large geospatial data and replicate workflows from ArcGIS Pro. In particular, this notebook shows how to perform spatial joins and counts using two geospatial datasets.\n\n\nDatasets and methods\nThe datasets were manually ingested as there is not a current integration between SIR or Open Data and the QESD platform. The protected areas of Queensland geospatial dataset was downloaded from SIR in geojson format and then read with geopandas locally to preserve the geometry. It was then saved as parquet format and uploaded to Blob Storage. Alternatively, the json file can be read directly with geopandas if the file is stored in Databricks Workspace, for example: example = gpd.read_file(‘/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/pa_all.json’). This dataset contains 1926 polygons (~ 7 MB).\nThe second file is the WildNet wildlife records (until 2023-09-21) sourced from Open data in gpkg format. The file was read with geopandas locally to preserve the geometry. The geometry column was transformed to string to make it compatible with the expected datatypes in parquet. It was then saved as parquet format and uploaded to blob Storage in the platform. This dataset contains ~2.2 million records (1.4 GB).\n\n\nFindings\nIt was possible to replicate the process of joining both datasets and producing total wildlife counts per protected area and per polygon using the Python module geopandas. Further granularity (filter, select) can be achieved based on future requirements. In summary, spatial joins, total wildlife counts in 1926 polygons and 1047 protected areas (from ~1770 to 2023-09-21) including those with multipolygons were performed.\n\n\nPrerequisites\n\nFamiliarity with spatial data structures and objects\nFamiliarity with Databricks\nUnderstanding of Azure Data Lake Storage (ADLS)\nFamiliarity with Python\nBasic understanding of Spark\n\n\n\nSetup\n\nMake sure you have access to a Databricks environment in the QESD platform.\nNavigate to sbox-databricks-prd\nNavigate to Workspace and open the folder platform_repo/geospatial. This folder contains several examples demonstrating the use of Azure Databricks for different geospatial analysis and operations in vector data, including building choropleth maps, spatial joins, counts and calculation of polygon areas in small and large datasets.\nStart a cluster. We recommend ‘singlenode’ as we do not need distributed computing and our datasets are small.\n\n\n# Install modules\n\n%pip install geopandas==0.14.0 # Pandas equivalent to process geospatial data\n%pip install folium==0.14.0 # To produce interactive maps\n\n\n# These libraries are pre-installed in a Databricks 12.2 LTS runtime cluster (except geopandas and folium)\n\nimport geopandas as gpd # Pandas equivalent to process geospatial data\nfrom shapely.geometry import Polygon, Point # To create geometries \nimport folium # To produce interactive maps\nfrom json import loads  # To load json files\nfrom shapely import wkt # To convert wkt geometries\nimport pandas as pd # To process data    \nimport os # To interact with the operating system\nfrom folium import plugins # To produce interactive maps\n\n\n# Import datasets from Azure Data Lake Storage (ADLS)\n\n# Get the environment name from the system environment variables\n\n\nenv = os.getenv('tfenvironmentnameshort')\n\n# Load the 'pa_all_srt.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n\nprotected_areas = (\n  spark.read\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pa_all_srt.parquet')\n)\n\n# Load the 'wildnet_pp_str.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n# The 'header' option is set to 'true' which means the first row of the dataset is considered as the header\n\nwild = (\n  spark.read\n    .option(\"header\", \"true\")\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/wildnet_pp_str.parquet')\n)\n\n\n# Convert Spark DataFrames to pandas DataFrames \n# This is necessary to convert the geometries from wkt to shapely geometries\n\npa_pd = protected_areas.toPandas()\nwild_pd = wild.toPandas()\n\n# Convert pandas DataFrames to GeoPandas DataFrames\n# This is necessary to perform spatial operations\n\npa_gdf = gpd.GeoDataFrame(pa_pd)\nwild_gpd = gpd.GeoDataFrame(wild_pd)\n\n\n# Define geometries\n\n# The 'geometry' column in the original data is a string\n# We need to convert this string representation of geometry into a Shapely geometry object\n# The 'from_wkt' function from GeoPandas is used for this conversion\n\npa_gdf['geometry2'] = gpd.GeoSeries.from_wkt(pa_gdf['geometry'])\n\n# Now, we initialize a new GeoDataFrame using the updated 'geometry2' column as the geometry\n# Note: 'geometry' is the default geometry column name in GeoPandas\npa_gdf = gpd.GeoDataFrame(pa_gdf).set_geometry(\"geometry2\")\n\n# replace string geometry representations with shapely geometries\nwild_gpd['geometry2'] = gpd.GeoSeries.from_wkt(wild_gpd['geometry'])\n\n# We repeat the same steps for the 'wild' dataset\n\nwild_gpd= gpd.GeoDataFrame(wild_gpd).set_geometry(\"geometry2\")\n\n\n# Perform spatial join on right to retain all records from observation points\n# The 'contains' operation is used to find all protected areas that contain the observation points\n\njoined_inner = gpd.sjoin(pa_gdf, wild_gpd, how='right', op='contains')\n\n\n# Count by protected area and polygon\n# The 'groupby' function is used to group the data by protected area and polygon\n\npoint_count = joined_inner.groupby(['geometry_left', 'estatename']).size().reset_index(name='total number of sightings')\n\n\n# Rejoin with protected areas\n# The 'merge' function is used to join the 'point_count' and 'pa_gdf' GeoDataFrames\nmerged_counts_pa = point_count.merge(pa_gdf, left_on = ['geometry_left'], right_on= ['geometry'], how = 'outer')\n\n# Convert to dataframe\n# The 'set_geometry' function is used to set the 'geometry2' column as the geometry column\nmerged_gdf = gpd.GeoDataFrame(merged_counts_pa).set_geometry('geometry2')\n\n\n# Calculate total number of sightings per protected area\n# The 'groupby' function is used to group the data by protected area\n# The 'aggregate' function is used to calculate the sum of the 'total number of sightings' column\n# The 'rename' function is used to rename the 'total number of sightings' column to 'aggregated sightings'\n\naggregation_functions = {'total number of sightings': 'sum'}\nmerged_agg = merged_gdf.groupby(merged_gdf['estatename_y']).aggregate(aggregation_functions)\nmerged_agg = merged_agg.rename(columns={\"total number of sightings\": \"aggregated sightings\"})\n\n\n# Set correct indexes to join aggregations with previous dataframe\n# The 'set_index' function is used to set the 'estatename_y' column as the index\n# The 'join' function is used to join the 'merged_gdf_index' and 'merged_agg' GeoDataFrames\n# The 'reset_index' function is used to reset the index of the GeoDataFrame\n\nmerged_gdf_index = merged_gdf.set_index('estatename_y')\nmerged = merged_gdf_index.join(merged_agg).reset_index()\n\n\n# Merge resulting dataframe with raw protected areas to include polygons with zero sightings\n# The 'merge' function is used to join the 'merged' and 'pa_gdf' GeoDataFrames\n# The 'left_on' and 'right_on' parameters are used to specify the join columns\n\nfinal_merged = merged.merge(pa_gdf, left_on= 'geometry2', right_on= 'geometry2', how = 'outer')\n\n# Include polygons with zero sighthings. Set zero in the polygon count as they show as NAs and replace NA's in aggregated sightings in those polygons with the value already calculated.\n# The 'fillna' function is used to replace all NaN values with 0\n# The 'groupby' function is used to group the data by protected area\n# The 'apply' function is used to apply a lambda function to the 'aggregated sightings' column\n# The lambda function is used to replace all NaN values with the maximum value in the 'aggregated sightings' column\n\nfinal_merged['total number of sightings'] = final_merged['total number of sightings'].fillna(0)\nfinal_merged['aggregated sightings'] = final_merged.groupby('estatename')['aggregated sightings'].apply(lambda x: x.fillna(x.max()))\nfinal_merged['aggregated sightings'] = final_merged['aggregated sightings'].fillna(0)\nfinal_merged = final_merged.drop(['estatename_x', 'geometry_x', 'geometry_y', 'geometry_left', 'estatename_y'], axis=1)\n\n\n# Test with three protected areas to compare with ArcGIS Pro\n# The 'loc' function is used to select the rows where the 'estatename' column is equal to 'Great Sandy National Park', 'Minerva Hills National Park', or 'Tinana Creek Conservation Park'\n\nfiltered_df = final_merged.loc[(final_merged['estatename'] == 'Great Sandy National Park') | (final_merged['estatename'] == 'Minerva Hills National Park') | (final_merged['estatename'] == 'Tinana Creek Conservation Park')]\n\nfiltered_df\n\n\n# Use folium to create a map. Only three protected areas are included for demonstration\n# The 'Map' function is used to create a map\n# The 'location' parameter is used to specify the center of the map\n# The 'zoom_start' parameter is used to specify the zoom level of the map\n\nm = folium.Map(location = [-24.0807, 148.0641], zoom_start = 6)\n\n\n# Use subset of main dataframe to build map\n\nfinal_merged_gdf = gpd.GeoDataFrame(filtered_df.set_geometry('geometry2')) \nfinal_merged_gdf = final_merged_gdf.set_crs('EPSG:4283')\n\n\n# Convert to geojson so plugin from folium can be used\n# The 'to_json' function is used to convert the GeoDataFrame to a GeoJSON object\n\npoints_gjson= folium.features.GeoJson(final_merged_gdf, name=\"wildlife counts\")\npoints_gjson.add_to(m)\n\n\n# Use folium plugin to create tooltip\n# The 'plugins.MarkerCluster' function is used to create a marker cluster\n# The 'add_to' function is used to add the marker cluster to the map\nfolium.features.GeoJson(final_merged_gdf,  \n                        name='Labels',\n                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n                        tooltip=folium.features.GeoJsonTooltip(fields=['aggregated sightings', 'estatename', 'total number of sightings'], \n                                                                aliases = ['Total number of sightings in protected area', 'Protected area', 'Number of sightings in polygon'],\n                                                                labels=True,\n                                                                sticky=False\n                                                                            )\n                       ).add_to(m)\n\nm\n\n\n# Save map to workspace - replace with your own path\n\n#m.save(outfile='/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/sandy_minerva_tinana.html')\n\n\n# Check number of polygons is correct\n# The 'count' function is used to count the number of rows in the GeoDataFrame\n\n# To store in parquet - geometry column needs to be changed\n\nmerged_pq = final_merged.copy()\n\nmerged_pq['geometry2'] = merged_pq['geometry2'].astype(str)\n\nmerged_pq = merged_pq.rename(columns = {'total number of sightings': 'number of sightings in polygon', 'aggregated sightings':'total number of sightings in protected area', 'geometry2': 'geometry'})\n\nmerged_pq.count()\n\n\n# Browse total number of sightings per protected area\n# The 'drop_duplicates' function is used to drop duplicate rows based on the 'estatename' and 'total number of sightings in protected area' columns\n# The 'createDataFrame' function is used to create a Spark DataFrame from a pandas DataFrame\n\n\nmerged_pq_sum = merged_pq[['estatename', 'total number of sightings in protected area']].drop_duplicates()\ndisplay(spark.createDataFrame(merged_pq_sum))\n\n\n# To store csv\n  \n #spark.createDataFrame(test).repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial/sandy_minerva_test.csv')\n\n\n# Write to blob \n# The 'write' function is used to write the Spark DataFrame to the specified file path\n\nspark.createDataFrame(merged_pq).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pr_areas_counts.parquet')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorials demonstrating the use of cloud tools using the QESD CLoud Platform",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the implementation of the QESD Cloud Platform, and are available for self-paced learning."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Datasets\nSlides"
  }
]