[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "cloud_concepts/lesson_1.html",
    "href": "cloud_concepts/lesson_1.html",
    "title": "Lesson 1: Physical and Logical Representation of Data (Files vs Views)",
    "section": "",
    "text": "Understand the concept of physical storage of data.\nUnderstand the concept of logical representation of data.\nUnderstand the difference between physical storage and logical representation.\n\n\n\n\n\nDefine and explain the concept of physical storage using platform storage accounts as examples.\nDefine and explain the concept of logical representation using views in Synapse as examples.\nDifferentiate between files in their raw formats in physical storage and cleaned, wrangled files in logical representation.\n\n\n\n\nPhysical data storage refers to the tangible means by which data is stored in digital form, on devices like hard drives, Flash storage, memory cards, DVDs, etc. When relating this to a storage account platform such as Sandbox Lake Broad, the data is physically stored as files within the cloud storage. These files are kept in containers, which form the primary component of the platform’s data storage architecture. Data here is raw and untransformed, thus may require additional processing steps for optimal use.\n\n\n\nContrary to physical data storage, logical representation of data abstracts the underlying complexity of physical storage. It concerns the way data is perceived from a user’s point of view, invariably hiding intricate lower-level details. For instance, in Synapse, users interact with a logical representation of the data - a structured, cleaned, and organised ‘view’ - rather than dealing directly with the physical storage. The underlying technology enables users to manipulate, search, and retrieve the data without needing extensive comprehension of how and where the data is physically stored.\n\n\n\nPhysical and logical data representations stand as complementary aspects of data storage and management.\nPhysical storage pertains to raw data files kept on some hardware or cloud-based platform. It does not concern itself with data’s meaningfulness or organisation - it’s fundamentally about ensuring data is stored, intact and accessible when required.\nOn the other hand, logical representation is about bringing ‘sense’ and ‘meaning’ to that stored data. It presents the data in an organised, often simplified form, making it easy for users to interact with and utilise the data.\nThink of physical representation as the warehouse storing boxes of raw materials (data), and the logical representation as the display in a store - sorted, arranged, and ready for consumers (users) to browse and purchase.\nRemember that comprehending both concepts is vital in understanding data management as a whole. They form the building blocks of data storage; understanding their synergy will guide you in leveraging the power of data effectively and efficiently.\nWe hope this lesson provided a foundational understanding of physical and logical data representation. Stay tuned for our upcoming lessons, which will delve deeper into different aspects of data management."
  },
  {
    "objectID": "cloud_concepts/lesson_1.html#physical-data-storage",
    "href": "cloud_concepts/lesson_1.html#physical-data-storage",
    "title": "Lesson 1: Physical and Logical Representation of Data (Files vs Views)",
    "section": "",
    "text": "Physical data storage refers to the tangible means by which data is stored in digital form, on devices like hard drives, Flash storage, memory cards, DVDs, etc. When relating this to a storage account platform such as Sandbox Lake Broad, the data is physically stored as files within the cloud storage. These files are kept in containers, which form the primary component of the platform’s data storage architecture. Data here is raw and untransformed, thus may require additional processing steps for optimal use."
  },
  {
    "objectID": "cloud_concepts/lesson_1.html#logical-representation-of-data",
    "href": "cloud_concepts/lesson_1.html#logical-representation-of-data",
    "title": "Lesson 1: Physical and Logical Representation of Data (Files vs Views)",
    "section": "",
    "text": "Contrary to physical data storage, logical representation of data abstracts the underlying complexity of physical storage. It concerns the way data is perceived from a user’s point of view, invariably hiding intricate lower-level details. For instance, in Synapse, users interact with a logical representation of the data - a structured, cleaned, and organised ‘view’ - rather than dealing directly with the physical storage. The underlying technology enables users to manipulate, search, and retrieve the data without needing extensive comprehension of how and where the data is physically stored."
  },
  {
    "objectID": "cloud_concepts/lesson_1.html#comparing-physical-and-logical-representations",
    "href": "cloud_concepts/lesson_1.html#comparing-physical-and-logical-representations",
    "title": "Lesson 1: Physical and Logical Representation of Data (Files vs Views)",
    "section": "",
    "text": "Physical and logical data representations stand as complementary aspects of data storage and management.\nPhysical storage pertains to raw data files kept on some hardware or cloud-based platform. It does not concern itself with data’s meaningfulness or organisation - it’s fundamentally about ensuring data is stored, intact and accessible when required.\nOn the other hand, logical representation is about bringing ‘sense’ and ‘meaning’ to that stored data. It presents the data in an organised, often simplified form, making it easy for users to interact with and utilise the data.\nThink of physical representation as the warehouse storing boxes of raw materials (data), and the logical representation as the display in a store - sorted, arranged, and ready for consumers (users) to browse and purchase.\nRemember that comprehending both concepts is vital in understanding data management as a whole. They form the building blocks of data storage; understanding their synergy will guide you in leveraging the power of data effectively and efficiently.\nWe hope this lesson provided a foundational understanding of physical and logical data representation. Stay tuned for our upcoming lessons, which will delve deeper into different aspects of data management."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud materials",
    "section": "",
    "text": "Main description"
  },
  {
    "objectID": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "href": "cloud_concepts/QLD Protected areas and wildlife spatial joins and counts.html",
    "title": "Description",
    "section": "",
    "text": "This notebook is part of a proof of concept developed to test the capabilities in the QESD platform to process large geospatial data and replicate workflows from ArcGIS Pro. In particular, this notebook shows how to perform spatial joins and counts using two geospatial datasets.\n\nDatasets and methods\nThe datasets were manually ingested as there is not a current integration between SIR or Open Data and the QESD platform. The protected areas of Queensland geospatial dataset was downloaded from SIR in geojson format and then read with geopandas locally to preserve the geometry. It was then saved as parquet format and uploaded to Blob Storage. Alternatively, the json file can be read directly with geopandas if the file is stored in Databricks Workspace, for example: example = gpd.read_file(‘/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/pa_all.json’). This dataset contains 1926 polygons (~ 7 MB).\nThe second file is the WildNet wildlife records (until 2023-09-21) sourced from Open data in gpkg format. The file was read with geopandas locally to preserve the geometry. The geometry column was transformed to string to make it compatible with the expected datatypes in parquet. It was then saved as parquet format and uploaded to blob Storage in the platform. This dataset contains ~2.2 million records (1.4 GB).\n\n\nFindings\nIt was possible to replicate the process of joining both datasets and producing total wildlife counts per protected area and per polygon using the Python module geopandas. Further granularity (filter, select) can be achieved based on future requirements. In summary, spatial joins, total wildlife counts in 1926 polygons and 1047 protected areas (from ~1770 to 2023-09-21) including those with multipolygons were performed.\n\n\nPrerequisites\n\nFamiliarity with spatial data structures and objects\nFamiliarity with Databricks\nUnderstanding of Azure Data Lake Storage (ADLS)\nFamiliarity with Python\nBasic understanding of Spark\n\n\n\nSetup\n\nMake sure you have access to a Databricks environment in the QESD platform.\nNavigate to sbox-databricks-prd\nNavigate to Workspace and open the folder platform_repo/geospatial. This folder contains several examples demonstrating the use of Azure Databricks for different geospatial analysis and operations in vector data, including building choropleth maps, spatial joins, counts and calculation of polygon areas in small and large datasets.\nStart a cluster. We recommend ‘singlenode’ as we do not need distributed computing and our datasets are small.\n\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“a45447eb-a203-45a0-9ba2-f7c28974b37b”,“showTitle”:false,“title”:““}’}\n# Install modules\n\n%pip install geopandas==0.14.0 # Pandas equivalent to process geospatial data\n%pip install folium==0.14.0 # To produce interactive maps\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“43215939-2e57-48de-9475-da3a0f5ada45”,“showTitle”:false,“title”:““}’}\n# These libraries are pre-installed in a Databricks 12.2 LTS runtime cluster (except geopandas and folium)\n\nimport geopandas as gpd # Pandas equivalent to process geospatial data\nfrom shapely.geometry import Polygon, Point # To create geometries \nimport folium # To produce interactive maps\nfrom json import loads  # To load json files\nfrom shapely import wkt # To convert wkt geometries\nimport pandas as pd # To process data    \nimport os # To interact with the operating system\nfrom folium import plugins # To produce interactive maps\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“0b3fb41e-ad65-4cf1-ba4a-5a8b0155c156”,“showTitle”:false,“title”:““}’}\n# Import datasets from Azure Data Lake Storage (ADLS)\n\n# Get the environment name from the system environment variables\n\n\nenv = os.getenv('tfenvironmentnameshort')\n\n# Load the 'pa_all_srt.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n\nprotected_areas = (\n  spark.read\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pa_all_srt.parquet')\n)\n\n# Load the 'wildnet_pp_str.parquet' dataset from ADLS into a Spark DataFrame\n# The file path is constructed using the environment name\n# The 'header' option is set to 'true' which means the first row of the dataset is considered as the header\n\nwild = (\n  spark.read\n    .option(\"header\", \"true\")\n    .format(\"parquet\")\n    .load(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/wildnet_pp_str.parquet')\n)\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“b579c062-e32d-4d4e-b4ef-8dd7a1a06901”,“showTitle”:false,“title”:““}’}\n# Convert Spark DataFrames to pandas DataFrames \n# This is necessary to convert the geometries from wkt to shapely geometries\n\npa_pd = protected_areas.toPandas()\nwild_pd = wild.toPandas()\n\n# Convert pandas DataFrames to GeoPandas DataFrames\n# This is necessary to perform spatial operations\n\npa_gdf = gpd.GeoDataFrame(pa_pd)\nwild_gpd = gpd.GeoDataFrame(wild_pd)\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“e327a4d4-c9b5-4135-8c1e-e8863775d44b”,“showTitle”:false,“title”:““}’}\n# Define geometries\n\n# The 'geometry' column in the original data is a string\n# We need to convert this string representation of geometry into a Shapely geometry object\n# The 'from_wkt' function from GeoPandas is used for this conversion\n\npa_gdf['geometry2'] = gpd.GeoSeries.from_wkt(pa_gdf['geometry'])\n\n# Now, we initialize a new GeoDataFrame using the updated 'geometry2' column as the geometry\n# Note: 'geometry' is the default geometry column name in GeoPandas\npa_gdf = gpd.GeoDataFrame(pa_gdf).set_geometry(\"geometry2\")\n\n# replace string geometry representations with shapely geometries\nwild_gpd['geometry2'] = gpd.GeoSeries.from_wkt(wild_gpd['geometry'])\n\n# We repeat the same steps for the 'wild' dataset\n\nwild_gpd= gpd.GeoDataFrame(wild_gpd).set_geometry(\"geometry2\")\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“03391762-d311-4c05-81e0-d09e75a52c2f”,“showTitle”:false,“title”:““}’}\n# Perform spatial join on right to retain all records from observation points\n# The 'contains' operation is used to find all protected areas that contain the observation points\n\njoined_inner = gpd.sjoin(pa_gdf, wild_gpd, how='right', op='contains')\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“0f472f73-f7ec-4c94-b6bb-888074e22a8b”,“showTitle”:false,“title”:““}’}\n# Count by protected area and polygon\n# The 'groupby' function is used to group the data by protected area and polygon\n\npoint_count = joined_inner.groupby(['geometry_left', 'estatename']).size().reset_index(name='total number of sightings')\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“b3d73438-2c40-4ecc-95eb-9cbb34cfaf15”,“showTitle”:false,“title”:““}’}\n# Rejoin with protected areas\n# The 'merge' function is used to join the 'point_count' and 'pa_gdf' GeoDataFrames\nmerged_counts_pa = point_count.merge(pa_gdf, left_on = ['geometry_left'], right_on= ['geometry'], how = 'outer')\n\n# Convert to dataframe\n# The 'set_geometry' function is used to set the 'geometry2' column as the geometry column\nmerged_gdf = gpd.GeoDataFrame(merged_counts_pa).set_geometry('geometry2')\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“1b7accce-ec5a-4994-bf09-4db8dfaf07b2”,“showTitle”:false,“title”:““}’}\n# Calculate total number of sightings per protected area\n# The 'groupby' function is used to group the data by protected area\n# The 'aggregate' function is used to calculate the sum of the 'total number of sightings' column\n# The 'rename' function is used to rename the 'total number of sightings' column to 'aggregated sightings'\n\naggregation_functions = {'total number of sightings': 'sum'}\nmerged_agg = merged_gdf.groupby(merged_gdf['estatename_y']).aggregate(aggregation_functions)\nmerged_agg = merged_agg.rename(columns={\"total number of sightings\": \"aggregated sightings\"})\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“f8b8e7d7-9b99-4b06-a575-2785effc6d8f”,“showTitle”:false,“title”:““}’}\n# Set correct indexes to join aggregations with previous dataframe\n# The 'set_index' function is used to set the 'estatename_y' column as the index\n# The 'join' function is used to join the 'merged_gdf_index' and 'merged_agg' GeoDataFrames\n# The 'reset_index' function is used to reset the index of the GeoDataFrame\n\nmerged_gdf_index = merged_gdf.set_index('estatename_y')\nmerged = merged_gdf_index.join(merged_agg).reset_index()\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“181626f3-cc2e-4fd1-a8cc-871af2117580”,“showTitle”:false,“title”:““}’}\n# Merge resulting dataframe with raw protected areas to include polygons with zero sightings\n# The 'merge' function is used to join the 'merged' and 'pa_gdf' GeoDataFrames\n# The 'left_on' and 'right_on' parameters are used to specify the join columns\n\nfinal_merged = merged.merge(pa_gdf, left_on= 'geometry2', right_on= 'geometry2', how = 'outer')\n\n# Include polygons with zero sighthings. Set zero in the polygon count as they show as NAs and replace NA's in aggregated sightings in those polygons with the value already calculated.\n# The 'fillna' function is used to replace all NaN values with 0\n# The 'groupby' function is used to group the data by protected area\n# The 'apply' function is used to apply a lambda function to the 'aggregated sightings' column\n# The lambda function is used to replace all NaN values with the maximum value in the 'aggregated sightings' column\n\nfinal_merged['total number of sightings'] = final_merged['total number of sightings'].fillna(0)\nfinal_merged['aggregated sightings'] = final_merged.groupby('estatename')['aggregated sightings'].apply(lambda x: x.fillna(x.max()))\nfinal_merged['aggregated sightings'] = final_merged['aggregated sightings'].fillna(0)\nfinal_merged = final_merged.drop(['estatename_x', 'geometry_x', 'geometry_y', 'geometry_left', 'estatename_y'], axis=1)\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“7306ca7b-7f6c-4fa3-8a66-bc9668474f69”,“showTitle”:false,“title”:““}’}\n# Test with three protected areas to compare with ArcGIS Pro\n# The 'loc' function is used to select the rows where the 'estatename' column is equal to 'Great Sandy National Park', 'Minerva Hills National Park', or 'Tinana Creek Conservation Park'\n\nfiltered_df = final_merged.loc[(final_merged['estatename'] == 'Great Sandy National Park') | (final_merged['estatename'] == 'Minerva Hills National Park') | (final_merged['estatename'] == 'Tinana Creek Conservation Park')]\n\nfiltered_df\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“b01e02d4-acb5-49ca-b99a-101bfcd6a5d5”,“showTitle”:false,“title”:““}’}\n# Use folium to create a map. Only three protected areas are included for demonstration\n# The 'Map' function is used to create a map\n# The 'location' parameter is used to specify the center of the map\n# The 'zoom_start' parameter is used to specify the zoom level of the map\n\nm = folium.Map(location = [-24.0807, 148.0641], zoom_start = 6)\n\n\n# Use subset of main dataframe to build map\n\nfinal_merged_gdf = gpd.GeoDataFrame(filtered_df.set_geometry('geometry2')) \nfinal_merged_gdf = final_merged_gdf.set_crs('EPSG:4283')\n\n\n# Convert to geojson so plugin from folium can be used\n# The 'to_json' function is used to convert the GeoDataFrame to a GeoJSON object\n\npoints_gjson= folium.features.GeoJson(final_merged_gdf, name=\"wildlife counts\")\npoints_gjson.add_to(m)\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“03321333-4846-4623-aeff-a86601bfaa6e”,“showTitle”:false,“title”:““}’}\n# Use folium plugin to create tooltip\n# The 'plugins.MarkerCluster' function is used to create a marker cluster\n# The 'add_to' function is used to add the marker cluster to the map\nfolium.features.GeoJson(final_merged_gdf,  \n                        name='Labels',\n                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n                        tooltip=folium.features.GeoJsonTooltip(fields=['aggregated sightings', 'estatename', 'total number of sightings'], \n                                                                aliases = ['Total number of sightings in protected area', 'Protected area', 'Number of sightings in polygon'],\n                                                                labels=True,\n                                                                sticky=False\n                                                                            )\n                       ).add_to(m)\n\nm\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“6afc3bf8-e255-4de9-b83d-44897e7f0084”,“showTitle”:false,“title”:““}’}\n# Save map to workspace - replace with your own path\n\n#m.save(outfile='/Workspace/Users/maria.riveraaraya@des.qld.gov.au/spatial autocorrelation/sandy_minerva_tinana.html')\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“6a02adb3-71b4-4e21-a3f3-37034bdc304f”,“showTitle”:false,“title”:““}’}\n# Check number of polygons is correct\n# The 'count' function is used to count the number of rows in the GeoDataFrame\n\n# To store in parquet - geometry column needs to be changed\n\nmerged_pq = final_merged.copy()\n\nmerged_pq['geometry2'] = merged_pq['geometry2'].astype(str)\n\nmerged_pq = merged_pq.rename(columns = {'total number of sightings': 'number of sightings in polygon', 'aggregated sightings':'total number of sightings in protected area', 'geometry2': 'geometry'})\n\nmerged_pq.count()\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“72a9790c-344c-47e6-843f-11c9cf268eed”,“showTitle”:false,“title”:““}’}\n# Browse total number of sightings per protected area\n# The 'drop_duplicates' function is used to drop duplicate rows based on the 'estatename' and 'total number of sightings in protected area' columns\n# The 'createDataFrame' function is used to create a Spark DataFrame from a pandas DataFrame\n\n\nmerged_pq_sum = merged_pq[['estatename', 'total number of sightings in protected area']].drop_duplicates()\ndisplay(spark.createDataFrame(merged_pq_sum))\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“f3c1d224-64c5-4183-9d38-fd3293e0aecc”,“showTitle”:false,“title”:““}’}\n# To store csv\n  \n #spark.createDataFrame(test).repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial/sandy_minerva_test.csv')\n:::\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“1fe7f1f5-687d-40e9-9d55-2a98d1fb7d1e”,“showTitle”:false,“title”:““}’}\n# Write to blob \n# The 'write' function is used to write the Spark DataFrame to the specified file path\n\nspark.createDataFrame(merged_pq).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@sboxlake{env}.dfs.core.windows.net/geospatial_test/pr_areas_counts.parquet')\n:::"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Datasets\nSlides"
  }
]